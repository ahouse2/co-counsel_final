---
services:
  # Include all existing services from infra/docker-compose.yml
  api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    environment:
      MODEL_PROVIDERS_PRIMARY: ${MODEL_PROVIDERS_PRIMARY:-gemini}
      MODEL_PROVIDERS_SECONDARY: ${MODEL_PROVIDERS_SECONDARY:-openai}
      DEFAULT_CHAT_MODEL: ${DEFAULT_CHAT_MODEL:-gemini-2.5-flash}
      DEFAULT_EMBEDDING_MODEL: ${DEFAULT_EMBEDDING_MODEL:-text-embedding-004}
      DEFAULT_VISION_MODEL: ${DEFAULT_VISION_MODEL:-gemini-2.5-flash}
      NEBULA_GRAPH_HOST: nebula-graphd
      NEBULA_GRAPH_PORT: 9669
      NEBULA_GRAPH_USER: root
      NEBULA_GRAPH_PASSWORD: nebula
      QDRANT_URL: http://qdrant:6333
      VECTOR_DIR: /data/vector
      VOICE_SESSIONS_DIR: /data/voice/sessions
      VOICE_CACHE_DIR: /models
      DOCUMENT_STORAGE_PATH: /var/cocounsel/documents
      GRAPH_SNAPSHOT_PATH: /var/cocounsel/graphs
      TELEMETRY_BUFFER_PATH: /var/cocounsel/telemetry
      BILLING_USAGE_PATH: /data/billing/usage.json
      BILLING_DEFAULT_PLAN: ${BILLING_DEFAULT_PLAN:-community}
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-false}
      TELEMETRY_OTLP_ENDPOINT: ${TELEMETRY_OTLP_ENDPOINT:-http://otel-collector:4317}
      TELEMETRY_OTLP_INSECURE: ${TELEMETRY_OTLP_INSECURE:-true}
      TELEMETRY_ENVIRONMENT: ${TELEMETRY_ENVIRONMENT:-community}
      STT_SERVICE_URL: ${STT_SERVICE_URL:-http://stt:9000}
      TTS_SERVICE_URL: ${TTS_SERVICE_URL:-http://tts:5002}
      HUGGINGFACE_HUB_CACHE: /var/cocounsel/models/huggingface
      WHISPER_MODEL_PATH: /var/cocounsel/models/whisper
      TTS_MODEL_PATH: /var/cocounsel/models/tts
    ports:
      - "8000:8000"
    depends_on:
      - nebula-graphd
      - nebula-metad
      - nebula-storaged
      - qdrant
      - postgres
    networks:
      - backend
    volumes:
      - api_data:/data
      - voice_models:/models
      - ./var/storage/documents:/var/cocounsel/documents
      - ./var/storage/graphs:/var/cocounsel/graphs
      - ./var/storage/telemetry:/var/cocounsel/telemetry
      - ./var/models/huggingface:/var/cocounsel/models/huggingface
      - ./var/models/whisper:/var/cocounsel/models/whisper
      - ./var/models/tts:/var/cocounsel/models/tts

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        VITE_API_BASE_URL: ${VITE_API_BASE_URL:-http://localhost:8000}
    ports:
      - "8080:80"
    networks:
      - backend
    depends_on:
      - api

  nebula-metad:
    image: vesoft/nebula-metad:v3.6.0
    container_name: nebula-metad
    ports:
      - "9559:9559"
    volumes:
      - nebula_metad_data:/data
    networks:
      - backend
    command:
      - --port=9559
      - --ws_http_port=19559
      - --data_path=/data/meta
      - --log_dir=/data/meta/logs
      - --v=0
      - --minloglevel=0
      - --max_log_size=100
      - --log_reserve_hours=72
      - --num_io_threads=1
      - --num_worker_threads=1
      - --meta_server_addrs=nebula-metad:9559

  nebula-storaged:
    image: vesoft/nebula-storaged:v3.6.0
    container_name: nebula-storaged
    ports:
      - "9779:9779"
    volumes:
      - nebula_storaged_data:/data
    networks:
      - backend
    depends_on:
      - nebula-metad
    command:
      - --port=9779
      - --ws_http_port=19779
      - --data_path=/data/storage
      - --log_dir=/data/storage/logs
      - --v=0
      - --minloglevel=0
      - --max_log_size=100
      - --log_reserve_hours=72
      - --num_io_threads=1
      - --num_worker_threads=1
      - --meta_server_addrs=nebula-metad:9559

  nebula-graphd:
    image: vesoft/nebula-graphd:v3.6.0
    container_name: nebula-graphd
    ports:
      - "9669:9669"
      - "19669:19669"
    networks:
      - backend
    depends_on:
      - nebula-metad
      - nebula-storaged
    command:
      - --port=9669
      - --ws_http_port=19669
      - --log_dir=/data/graph/logs
      - --v=0
      - --minloglevel=0
      - --max_log_size=100
      - --log_reserve_hours=72
      - --num_io_threads=1
      - --num_worker_threads=1
      - --meta_server_addrs=nebula-metad:9559

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - backend

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-cocounsel}
      POSTGRES_USER: ${POSTGRES_USER:-cocounsel}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-securepassword}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - backend

  stt:
    image: fedirz/faster-whisper-server:latest-cuda
    runtime: runc
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-0}
              capabilities: [gpu]
    environment:
      ASR_MODEL: ${STT_MODEL_NAME:-openai/whisper-small}
      ASR_ENGINE: faster_whisper
      ASR_BEAM_SIZE: 5
      ASR_DEVICE: ${STT_DEVICE:-cpu}
      HUGGINGFACE_HUB_CACHE: /models/huggingface
      ASR_OUTPUT_LANGUAGE: ${STT_OUTPUT_LANGUAGE:-en}
    ports:
      - "9000:9000"
    volumes:
      - ./var/models/huggingface:/models/huggingface
      - ./var/models/whisper:/models/whisper
    networks:
      - backend

  tts:
    image: rhasspy/larynx:latest
    runtime: runc
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-0}
              capabilities: [gpu]
    environment:
      LARYNX_VOICE: ${TTS_VOICE:-en-us-blizzard_lessac}
      LARYNX_OUTPUT_DIR: /output
      HUGGINGFACE_HUB_CACHE: /models/huggingface
    ports:
      - "5002:5002"
    volumes:
      - ./var/models/huggingface:/models/huggingface
      - ./var/models/tts:/models/tts
      - ./var/audio:/output
    networks:
      - backend

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.98.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./infra/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"
      - "9464:9464"
    networks:
      - backend

  storage-backup:
    image: ghcr.io/offen/docker-volume-backup:latest
    environment:
      BACKUP_CRON: ${BACKUP_CRON_SCHEDULE:-0 3 * * *}
      BACKUP_FILENAME: full-stack
      BACKUP_PATH: /var/backups
      BACKUP_PRUNING_PREFIX: full-stack
      BACKUP_PRUNING_KEEP_DAYS: ${BACKUP_RETENTION_DAYS:-7}
      BACKUP_LATEST_SYMLINK: true
    volumes:
      - ./var/backups:/var/backups
      - ./var/storage/documents:/backup/documents:ro
      - ./var/storage/graphs:/backup/graphs:ro
      - ./var/storage/telemetry:/backup/telemetry:ro
    networks:
      - backend

volumes:
  nebula_graphd_data:
  nebula_metad_data:
  nebula_storaged_data:
  qdrant_data:
  api_data:
  voice_models:
  postgres_data:

networks:
  backend:
    driver: bridge
